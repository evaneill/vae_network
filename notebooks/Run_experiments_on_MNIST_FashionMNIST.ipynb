{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Run experiments on MNIST/FashionMNIST",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPkcLUJMLpNZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparameters\n",
        "alpha = -500 # alpha value used in Renyi alpha-divergence, ignored when model_type is vae/iwae/vrmax\n",
        "K = 5 # number of samples taken per input data point\n",
        "L = 2 # number of stochastic layers in network architecture; either 1 or 2\n",
        "\n",
        "# VAE, IWAE, VR-max and VR-alpha are described in the paper\n",
        "# general_alpha is the same as VR-alpha except that we backpropagate K samples instead of only one\n",
        "model_type = 'vralpha' # one of ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha']\n",
        "data_name = 'fashion' # one of ['mnist', 'fashion', 'fashionmnist']\n",
        "\n",
        "epochs = 501\n",
        "learning_rate = 1e-3\n",
        "\n",
        "log_interval = 5 # how frequently to log average training loss\n",
        "test_interval = 100 # how frequently to test\n",
        "train_batch_size = 100 # batch size during training\n",
        "test_batch_size = 32 # batch size used during testing, different than training because testing is done with K=5000\n",
        "\n",
        "seed = 1 # fixed seed\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g4jivVFmwX7W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import datetime\n",
        "import logging\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.utils.data\n",
        "from torch import nn, optim, Tensor as T\n",
        "from torch.autograd import detect_anomaly\n",
        "from torch.distributions.multinomial import Multinomial\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import save_image\n",
        "\n",
        "os.makedirs('results', exist_ok=True)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "assert(L in [1, 2]) # we only have networks with 1 or 2 stochastic layers\n",
        "assert(model_type in ['vae', 'iwae', 'vrmax', 'vralpha', 'general_alpha'])\n",
        "assert(not(alpha==1 and model_type in ['vralpha', 'general_alpha'])) # divide by 0 error otherwise\n",
        "assert(data_name in ['mnist', 'fashion', 'fashionmnist'])\n",
        "\n",
        "class mnist_omniglot_model1(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super(mnist_omniglot_model1, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc31 = nn.Linear(200, 50)\n",
        "        self.fc32 = nn.Linear(200, 50)\n",
        "\n",
        "        self.fc4 = nn.Linear(50, 200)\n",
        "        self.fc5 = nn.Linear(200, 200)\n",
        "        self.fc6 = nn.Linear(200, 784)\n",
        "\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        return self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "    def reparameterize(self, mu, logstd):\n",
        "        std = torch.exp(logstd)\n",
        "        eps = torch.randn_like(std)\n",
        "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
        "        return mu + eps*std\n",
        "\n",
        "    def decode(self, z):\n",
        "        h3 = torch.tanh(self.fc4(z))\n",
        "        h4 = torch.tanh(self.fc5(h3))\n",
        "        return torch.sigmoid(self.fc6(h4))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logstd = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logstd)\n",
        "        return self.decode(z), mu, logstd\n",
        "\n",
        "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
        "        # data = (B, 1, H, W)\n",
        "        B, _, H, W = data.shape\n",
        "\n",
        "        # Generate K copies of each observation. Each will get sampled once according to the generated distribution to generate a total of K observation samples\n",
        "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H*W)\n",
        "\n",
        "        # Retrieve the estimated mean and log(standard deviation) estimates from the posterior approximator\n",
        "        mu, logstd = model.encode(data_k_vec)\n",
        "\n",
        "        # Use the reparametrization trick to generate (mean)+(epsilon)*(standard deviation) for each sample of each observation\n",
        "        z = model.reparameterize(mu, logstd)\n",
        "\n",
        "        # Calculate log q(z|x) - how likely are the importance samples given the distribution that generated them?\n",
        "        log_q = compute_log_probabitility_gaussian(z, mu, logstd)\n",
        "\n",
        "        # Calculate log p(z) - how likely are the importance samples under the prior N(0,1) assumption?\n",
        "        log_p_z = compute_log_probabitility_gaussian(z, torch.zeros_like(z, requires_grad=False), torch.zeros_like(z, requires_grad=False))\n",
        "        \n",
        "        # Hand the samples to the decoder network and get a reconstruction of each sample.\n",
        "        decoded = model.decode(z)\n",
        "\n",
        "        # Calculate log p(x|z) with a bernoulli distribution - how likely are the recreations given the latents that generated them?\n",
        "        log_p = compute_log_probabitility_bernoulli(decoded, data_k_vec)\n",
        "\n",
        "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
        "        # log_p_z + log_p - log_q = log(p(z_i)p(x|z_i)/q(z_i|x)) = log(p(x,z_i)/q(z_i|x)) = L_VI\n",
        "        #   (for each importance sample i out of K for each observation)\n",
        "        if model_type == 'iwae' or test:\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(B, K)\n",
        "        \n",
        "        elif model_type =='vae':\n",
        "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(B*K, 1)*1/K\n",
        "        \n",
        "        elif model_type=='general_alpha' or model_type=='vralpha':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K) * (1-alpha)\n",
        "        \n",
        "        elif model_type == 'vrmax':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Take the max in each row, representing the maximum-weighted sample\n",
        "            log_w_matrix = (log_p_z + log_p - log_q).view(-1, K).max(axis=1, keepdim=True).values\n",
        "\n",
        "            # immediately return loss = -sum(L_alpha) over each observation\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
        "        # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
        "        log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
        "\n",
        "        # Exponentiate so that each term is [p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]) (no log)\n",
        "        ws_matrix = torch.exp(log_w_minus_max)\n",
        "\n",
        "        # Calculate normalized weights in each row. Max denominators cancel out!\n",
        "        # ws_norm = [p(z_i,x)/q(z_i|x)]/SUM([p(z_k,x)/q(z_k|x)])\n",
        "        ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
        "\n",
        "        if model_type == 'vralpha' and not test:\n",
        "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
        "            # So we make a distribution in each row\n",
        "            sample_dist = Multinomial(1, ws_norm)\n",
        "\n",
        "            # Then we choose a sample in each row acccording to this distribution\n",
        "            ws_sum_per_datapoint = log_w_matrix.gather(1, sample_dist.sample().argmax(1, keepdim=True))\n",
        "        else:\n",
        "            # For any other model, we're taking the full sum at this point\n",
        "            ws_sum_per_datapoint = torch.sum(log_w_matrix * ws_norm, 1)\n",
        "\n",
        "        if model_type in [\"general_alpha\", \"vralpha\"] and not test:\n",
        "            # For both VR-alpha and directly estimating L_alpha with a sum, we have to renormalize the sum with 1-alpha\n",
        "            ws_sum_per_datapoint /= (1 - alpha)\n",
        "\n",
        "        # Return a value of loss = -L_alpha as the batch sum.\n",
        "        loss = -torch.sum(ws_sum_per_datapoint)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Define the model\n",
        "class mnist_omniglot_model2(nn.Module):\n",
        "    def __init__(self, alpha):\n",
        "        super(mnist_omniglot_model2, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(784, 200)\n",
        "        self.fc2 = nn.Linear(200, 200)\n",
        "        self.fc31 = nn.Linear(200, 100)  # stochastic 1\n",
        "        self.fc32 = nn.Linear(200, 100)\n",
        "\n",
        "        self.fc4 = nn.Linear(100, 100)\n",
        "        self.fc5 = nn.Linear(100, 100)\n",
        "        self.fc61 = nn.Linear(100, 50)  # Innermost (stochastic 2)\n",
        "        self.fc62 = nn.Linear(100, 50)\n",
        "\n",
        "        self.fc7 = nn.Linear(50, 100)\n",
        "        self.fc8 = nn.Linear(100, 100)\n",
        "        self.fc81 = nn.Linear(100, 100)  # stochastic 1\n",
        "        self.fc82 = nn.Linear(100, 100)\n",
        "\n",
        "        self.fc9 = nn.Linear(100, 200)\n",
        "        self.fc10 = nn.Linear(200, 200)\n",
        "        self.fc11 = nn.Linear(200, 784)  # reconstruction\n",
        "\n",
        "        self.K = K\n",
        "        self.alpha = alpha\n",
        "\n",
        "    def encode(self, x):\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "        z1 = self.reparameterize(mu, log_std)\n",
        "        h3 = torch.tanh(self.fc4(z1))\n",
        "        h4 = torch.tanh(self.fc5(h3))\n",
        "\n",
        "        return self.fc61(h4), self.fc62(h4), [x, z1]\n",
        "\n",
        "    def reparameterize(self, mu, logstd, test=False):\n",
        "        std = torch.exp(logstd)\n",
        "        if test == True:\n",
        "            eps = torch.zeros_like(mu)\n",
        "        else:\n",
        "            eps = torch.randn_like(std)\n",
        "        # This is the reparametrization trick - represent the sample as a sum rather than black-box generated number\n",
        "        return mu + eps * std\n",
        "\n",
        "    def decode(self, z, test=False):\n",
        "        h5 = torch.tanh(self.fc7(z))\n",
        "        h6 = torch.tanh(self.fc8(h5))\n",
        "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
        "\n",
        "        z1 = self.reparameterize(mu, log_std, test=test)\n",
        "        h7 = torch.tanh(self.fc9(z1))\n",
        "        h8 = torch.tanh(self.fc10(h7))\n",
        "\n",
        "        return torch.sigmoid(self.fc11(h8))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu, logstd, _ = self.encode(x.view(-1, 784))\n",
        "        z = self.reparameterize(mu, logstd)\n",
        "        return self.decode(z), mu, logstd\n",
        "\n",
        "    def compute_loss_for_batch(self, data, model, K=K, test=False):\n",
        "        B, _, H, W = data.shape\n",
        "\n",
        "        # First repeat the observations K times, representing the data as a flat (M*K, # of pixels)\n",
        "        data_k_vec = data.repeat((1, K, 1, 1)).view(-1, H * W)\n",
        "\n",
        "        # Encode the model and retrieve estimated distribution parameters mu and log(standard deviation) for each sample of each observation\n",
        "        # z1 holds the latent samples generated at the first stochastic layer.\n",
        "        mu, log_std, [x, z1] = self.encode(data_k_vec)\n",
        "\n",
        "        # Sample from each observation's approximated latent distribution in each row (i.e. once for each of K importance samples, represented by rows)\n",
        "        # (this uses the reparametrization trick!)\n",
        "        z = model.reparameterize(mu, log_std)\n",
        "\n",
        "        # Calculate Log p(z) (prior) - how likely are these values given the prior assumption N(0,1)?\n",
        "        log_p_z = torch.sum(-0.5 * z ** 2, 1) - .5 * z.shape[1] * T.log(torch.tensor(2 * np.pi))\n",
        "\n",
        "        # Calculate q (z | h1) - how likely are the generated output latent samples given the distributions they came from?\n",
        "        log_qz_h1 = compute_log_probabitility_gaussian(z, mu, log_std)\n",
        "\n",
        "        # Re-Generate the mu and log_std that generated the first-layer latents z1\n",
        "        h1 = torch.tanh(self.fc1(x))\n",
        "        h2 = torch.tanh(self.fc2(h1))\n",
        "        mu, log_std = self.fc31(h2), self.fc32(h2)\n",
        "\n",
        "        # Calculate log q(h1|x) - how likely are the first-stochastic-layer latents given the distributions they come from?\n",
        "        log_qh1_x = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
        "\n",
        "        # Calculate the distribution parameters that generated the first-layer latents upon decoding\n",
        "        h5 = torch.tanh(self.fc7(z))\n",
        "        h6 = torch.tanh(self.fc8(h5))\n",
        "        mu, log_std = self.fc81(h6), self.fc82(h6)\n",
        "\n",
        "        # Calculate log p(h1|z) - how likely are the latents z1 under the parameters of the distribution here?\n",
        "        #   (This directly encourages the decoder to learn the inverse of the map h1->z)\n",
        "        log_ph1_z = compute_log_probabitility_gaussian(z1, mu, log_std)\n",
        "\n",
        "        # Finally calculate the reconstructed image\n",
        "        h7 = torch.tanh(self.fc9(z1))\n",
        "        h8 = torch.tanh(self.fc10(h7))\n",
        "        decoded = torch.sigmoid(self.fc11(h8))\n",
        "\n",
        "        # calculate log p(x | h1) - how likely is the reconstruction given the latent samples that generated it?\n",
        "        log_px_h1 = compute_log_probabitility_bernoulli(decoded, x)\n",
        "\n",
        "        # Begin calculating L_alpha depending on the (a) model type, and (b) optimization method\n",
        "        # log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x = \n",
        "        #           log([p(z0_i)p(x|z1_i)p(z1_i|z0_i)]/[q(z0_i|z1_i)q(z1_i|x)]) = log(p(x,z0_i,z1_i)/q(z0_i,z1_i|x)) = L_VI\n",
        "        #   (for each importance sample i out of K for each observation)\n",
        "        # Note that if test==True then we're always using the IWAE objective!\n",
        "        if model_type == 'iwae' or test == True:\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K)\n",
        "\n",
        "        elif model_type == 'vae':\n",
        "            # Don't reorder, and divide by K in anticipation of taking a batch sum of (1/K)*SUM(log(p(x,z)/q(z|x)))\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, 1) * 1 / K\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        elif model_type == 'general_alpha' or model_type == 'vralpha':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Multiply by (1-alpha) because (1-alpha)* log(p(x,z_i)/q(z_i|x)) =  log([p(x,z_i)/q(z_i|x)]^(1-alpha))\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K) * (1 - self.alpha)\n",
        "\n",
        "        elif model_type == 'vrmax':\n",
        "            # Re-order the entries so that each row holds the K importance samples for each observation\n",
        "            # Take the max in each row, representing the maximum-weighted sample, then immediately return batch sum loss -L_alpha\n",
        "            log_w_matrix = (log_p_z + log_ph1_z + log_px_h1 - log_qz_h1 - log_qh1_x).view(-1, K).max(axis=1,keepdim=True).values\n",
        "            return -torch.sum(log_w_matrix)\n",
        "\n",
        "        # Begin using the \"max trick\". Subtract the maximum log(*) sample value for each observation.\n",
        "        # log_w_minus_max = log([p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]))\n",
        "        log_w_minus_max = log_w_matrix - torch.max(log_w_matrix, 1, keepdim=True)[0]\n",
        "\n",
        "        # Exponentiate so that each term is [p(z_i,x)/q(z_i|x)] / max([p(z_k,x)/q(z_k|x)]) (no log)\n",
        "        ws_matrix = torch.exp(log_w_minus_max)\n",
        "\n",
        "        # Calculate normalized weights in each row. Max denominators cancel out!\n",
        "        # ws_norm = [p(z_i,x)/q(z_i|x)]/SUM([p(z_k,x)/q(z_k|x)])\n",
        "        ws_norm = ws_matrix / torch.sum(ws_matrix, 1, keepdim=True)\n",
        "\n",
        "        if model_type == 'vralpha' and not test:\n",
        "            # If we're specifically using a VR-alpha model, we want to choose a sample to backprop according to the values in ws_norm above\n",
        "            # So we make a distribution in each row\n",
        "            sample_dist = Multinomial(1, ws_norm)\n",
        "\n",
        "            # Then we choose a sample in each row acccording to this distribution\n",
        "            ws_sum_per_datapoint = log_w_matrix.gather(1, sample_dist.sample().argmax(1, keepdim=True))\n",
        "        else:\n",
        "            # For any other model, we're taking the full sum at this point\n",
        "            ws_sum_per_datapoint = torch.sum(log_w_matrix * ws_norm, 1)\n",
        "\n",
        "        if model_type in [\"general_alpha\", \"vralpha\"] and not test:\n",
        "            # For both VR-alpha and directly estimating L_alpha with a sum, we have to renormalize the sum with 1-alpha\n",
        "            ws_sum_per_datapoint /= (1 - alpha)\n",
        "\n",
        "        loss = -torch.sum(ws_sum_per_datapoint)\n",
        "\n",
        "        return loss\n",
        "\n",
        "# Compute N(obs| mu, sigma) for all K samples and sum over probabilities of the K samples\n",
        "def compute_log_probabitility_gaussian(obs, mu, logstd, axis=1):\n",
        "    return torch.sum(-0.5 * ((obs-mu) / torch.exp(logstd)) ** 2 - logstd, axis)-.5*obs.shape[1]*T.log(torch.tensor(2*np.pi))\n",
        "\n",
        "# Compute Ber(obs| theta) for all K samples and sum over probabilities of the K samples\n",
        "def compute_log_probabitility_bernoulli(theta, obs, axis=1):\n",
        "    # 1e-18 needed to avoid numerical errors\n",
        "    return torch.sum(obs*torch.log(theta+1e-18) + (1-obs)*torch.log(1-theta+1e-18), axis)\n",
        "\n",
        "# train and test functions\n",
        "def train(epoch):\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    for batch_idx, (data, labels) in enumerate(train_loader):\n",
        "        # (B, 1, F1, F2) (e.g. (128, 1, 28, 28) for MNIST with B=128)\n",
        "        data = data.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        loss = model.compute_loss_for_batch(data, model)\n",
        "        with detect_anomaly():\n",
        "            loss.backward()\n",
        "        train_loss += loss.item()\n",
        "        optimizer.step()\n",
        "\n",
        "    if epoch % log_interval == 0:\n",
        "        print(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "        logging.info(f'====> Epoch: {epoch} Average loss: {train_loss / len(train_loader.dataset):.4f}')\n",
        "\n",
        "def _test(epoch):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    with torch.no_grad():\n",
        "        for i, (data, labels) in enumerate(test_loader):\n",
        "            data = data.to(device)\n",
        "            recon_batch, mu, logvar = model(data)\n",
        "            loss = model.compute_loss_for_batch(data, model, K=5000,test=True)\n",
        "            test_loss += loss.item()\n",
        "            if i == 0:\n",
        "                # Visualizing reconstructions\n",
        "                n = min(data.size(0), 8)\n",
        "                comparison = torch.cat([data[:n],\n",
        "                                      recon_batch.view(test_batch_size, 1, 28, 28)[:n]])\n",
        "                save_image(comparison.cpu(),\n",
        "                         f'results/reconstruction_{model_type}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png',\n",
        "                           nrow=n)\n",
        "                # Visualizing random samples from the latent space\n",
        "                noise = torch.randn(64, 50).to(device)\n",
        "                sample = model.decode(noise).cpu()\n",
        "                save_image(sample.view(64, 1, 28, 28),\n",
        "                           f'results/sample_{model_type}_L={L}_{data_name}_alpha={alpha}_K={K}_epoch={epoch}.png')\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "    print(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
        "    logging.info(f'====> Epoch: {epoch} Test set loss: {test_loss:.4f}')\n",
        "    return test_loss\n",
        "\n",
        "def load_data_and_initialize_loaders(data_name, train_batch, test_batch):\n",
        "    data_name = data_name.lower()\n",
        "    kwargs = {'num_workers': 1, 'pin_memory': True}\n",
        "    if data_name == 'mnist':\n",
        "        train_data = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        test_data = datasets.MNIST('./data', train=False, transform=transforms.ToTensor())\n",
        "    elif data_name == 'fashion' or data_name == 'fashionmnist':\n",
        "        train_data = datasets.FashionMNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "        test_data = datasets.FashionMNIST('./data', train=False, transform=transforms.ToTensor())\n",
        "    train_loader = torch.utils.data.DataLoader(train_data, batch_size = train_batch, shuffle = True, ** kwargs)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size = test_batch, shuffle = True, ** kwargs)\n",
        "    return train_loader, test_loader\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    if L==1:\n",
        "        model = mnist_omniglot_model1(alpha).to(device)\n",
        "    else:\n",
        "        model = mnist_omniglot_model2(alpha).to(device)\n",
        "    train_loader, test_loader = load_data_and_initialize_loaders(data_name, train_batch_size, test_batch_size)\n",
        "    if torch.cuda.is_available():\n",
        "        print(\"Training on GPU\")\n",
        "        logging.info(\"Training on GPU\")\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    print(f'{datetime.datetime.now()} \\nStarting training')\n",
        "    logging.info(f'{datetime.datetime.now()} \\nStarting training')\n",
        "    for e in range(1, epochs+1):\n",
        "        train(e)\n",
        "        if e % test_interval == 0:\n",
        "            _test(e)\n",
        "    _test(epochs)\n",
        "    print(datetime.datetime.now())\n",
        "    logging.info(datetime.datetime.now())\n",
        "    print(\"Training finished\")\n",
        "    logging.info(\"Training finished\")\n",
        "    print(\"Saving model\")\n",
        "    torch.save(model.state_dict(),\n",
        "               f'models/{model_type}_L={L}_{data_name}_alpha={alpha}_K={K}_epochs={epochs}.pt')"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}